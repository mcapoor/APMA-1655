\documentclass[12pt]{article} 
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{letterpaper}
\usepackage{graphicx} 
\usepackage{parskip}
\usepackage{booktabs}
\usepackage{array} 
\usepackage{paralist} 
\usepackage{verbatim}
\usepackage{subfig}
\usepackage{fancyhdr}
\usepackage{sectsty}

\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt} 
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} 
\usepackage[titles,subfigure]{tocloft}
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} %

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{empheq}
\usepackage{xcolor}
\renewcommand{\L}[1]{\mathcal{L}\{#1\}}
\newcommand{\ans}[1]{\boxed{\text{#1}}}
\newcommand{\vecs}[1]{\langle #1\rangle}
\renewcommand{\hat}[1]{\widehat{#1}}
\newcommand{\F}[1]{\mathcal{F}(#1)}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\qed}{\quad \blacksquare}
\newcommand{\brak}[1]{\langle #1 \rangle}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\text{Var}\,}
\title{APMA 1655: Final Exam Review}
\date{}
\author{}
\begin{document}

\maketitle
\vspace*{-1in}
\tableofcontents
\section{Probability}
\subsection{Definitions}
\textbf{Random Event:} an event with more than one possible outcome of varying likelihoods where the true outcome is a priori unknown 

\textbf{Sample Space $\Omega$:} the set of all possible outcomes of an experiment

\textbf{Event:} Each subset E of $\Omega$

\textbf{Impossible Event:} the empty set $\emptyset$

\subsection{Set Operations}
Suppose $\Omega$ is a sample space and A and B are events $\{\omega \in \Omega: \omega \in A \text{ and } \omega \in B\}$

\textbf{Intersection ($A \cap B$):} A and B 

\textbf{Union ($A \cup B$):} A or B 

\textbf{Complement ($A^c$):} not A 

\textbf{De Morgan's Laws:}
\begin{align*}
    (A \cup B)^c &= A^c \cap B^c\\
    (A \cap B)^c &= A^c \cup B^c
\end{align*}

\subsubsection*{Infinite Sets}
\textbf{Infinite Intersection:} the collection of events that are in all the sets $A_1, \, ..., A_n$
\[\bigcap_{n=1}^\infty A_n = \{\omega \in \Omega : \omega \in A_n, \; \forall n = 1, 2, ...\}\]

\textbf{Infinite union:} the collection of events in at least one of the sets (``at least one of these events happens'')
\[\bigcup_{n=1}^\infty A_n = \{\omega \in \Omega: \exists i \;| \omega \in A_i\}\]

\subsection{Probability Space}
\subsubsection*{Definitions}
\textbf{Disjoint:} $A \cap B = \emptyset$

\textbf{Mutually disjoint:} all pairwise intersections of $A_1, \, ..., A_n$ are empty: $A_n \cup A_m = \emptyset \quad n\neq m$ 

\textbf{Probability $\P$:} a real-valued function $\P : \{\text{subsets of } \Omega\} \to \R$. This is often defined as 
\[\P(A) = \frac{\#A}{\#\Omega} \qquad A \subset \Omega\]

\textbf{Probability space:} The pair $(\Omega, \P)$ if $\P$ satisfies the following three axioms:
\begin{enumerate}
    \item $\P(A) \geq 0 \qquad \forall A \subset \Omega$
    \item $\P(\Omega) = 1$
    \item For any sequence of disjoint subsets $\{A_i\}_{i=1}^\infty$
    \[\P\left(\bigcup_{i=1}^\infty A_i\right) = \sum_{i=1}^\infty \P(A_i)\]
\end{enumerate}

\subsection{Properties of Probability}
\begin{enumerate}
    \item $\P(\emptyset) = 0$
    \item if $E_1 \cap E_2 =\emptyset$
    \[\P(E_1 \cup E_2) = \P(E_1) + \P(E_2)\]
    \item if $A, B \subset \Omega$ and $A \subset B$, then $\P(A) \leq \P(B)$
    \item $0 \leq \P(A) \leq 1$
    \item $\P(A^c) = 1- \P(A)$
    \item $\P(A \cup B) = \P(A) + \P(B) - \P(A \cap B)$
    \item \[\P\left(\bigcup_{n=1}^\infty A_n\right) \leq \sum_{n=1}^\infty \P(A_n)\]
\end{enumerate}

\subsection{Conditional Probability}
If $\P(B) > 0$, 
\[\P(A | B) = \frac{\P(A \cap B)}{\P(B)}\]

\textbf{Theorem:} If $(\Omega, \P)$ is a probability space, $B \subset \Omega$, $\P(B) > 0$, then $(\P(A | B), \Omega)$ is also a probability space

\textbf{Multiplication Law}
\[\P(A \cap B) = \P(A | B) \cdot \P(B)\]

\textbf{Partition:} $B_1, \, ..., B_n \subset \Omega$ if they are mutually disjoint and $\bigcup_{i=1}^n B_i = \Omega$

\textbf{The Law of Total Probability:} If $B_1, \, ..., B_n$ provide a partition of $\Omega$ 
\[\P(A) = \sum_{i=1}^n \P(A | B_i) \cdot \P(B_i)\]

\textbf{Corollary of the Law of Total Probability:} If $0 < \P(B) <1$, 
\[\P(A) = \P(A | B) \cdot \P(B) + \P(A |B^c) \cdot \P(B^c)\]

\textbf{Bayes' Rule}: Suppose $B_1, \,..., B_n$ partition $\Omega$ and $\P(B_i), \P(A) > 0 \quad (i \in [1, n])$. Then 
\[\P(B_i | A) = \frac{\P(A | B_i)\cdot \P(B_i)}{\sum_{j=1}^n \P(A | B_j) \cdot \P(B_j)}\qquad i = 1, 2, \,..., n\]

\textbf{Independence:} events that do not affect each other's outcomes:
\[\begin{cases}
    \P(A | B) = \P(A)\\
    \P(B | A) = \P(B)
\end{cases}\]

For independent events, 
\[\P(A \cap B) = \P(A) \cdot \P(B)\]

\textbf{Mutually independent:} if $\P(A_m \cap A_n) = \P(A_m) \cdot \P(A_n) \quad m\neq n$

\subsection{Random Variable}
\textbf{Definition:} On a probability space $(\Omega, \P)$, a real valued function $X : \Omega \to \R$ is a random variable

\textbf{Continuous Random Variable:} a random variable with a continuous CDF

\textbf{Discrete Random Variable:} a random variable with a discrete CDF

\textbf{Independent Random Vairbales:} $Y, Z$ on $(\Omega, \P)$ are independent if 
\[\P((Y \in A) \cap (Z \in B)) = \P(Y \in A) \cdot \P(Z\in B)\]
for any subsets $A, B \in \R$

\subsection{Cumulative Distribution Functions (CDF)}
\[F_X(x) = \P(\{\omega \in \Omega: X(\omega) \leq x\})\]


\textbf{Bernoulli Distribution:} $X \sim \text{Bernoulli}(p)$ if 
\[F_X(x) = \begin{cases}
    0 \qquad \qquad x <0\\
    1 - p \qquad \, 0 \leq x < 1\\
    1 \qquad \qquad x\geq 1 
\end{cases}\]

\subsubsection*{Properties of the CDF $F_X$}
\begin{enumerate}
    \item $F_X(x_1) \leq F_X(x_2) \qquad x_1 \leq x_2$
    \item \[\begin{cases}
        \lim_{x \to \; -\infty} F_X(x) = 0\\
        \lim_{x \to \; \infty} F_X(x) = 1
    \end{cases}\]
    \item $F_X$ is right continuous ($F_X(x_0) = \lim_{x\to x_0^+} F_X(x)$)
    \item $\P(X = x_0) = F_X(x_0) - \lim_{x\to x_0^-} F_X(x)$
    Note that this is zero if the CDF is continuous
\end{enumerate}

\textbf{Grand Theorem:} Given a CDF, there exist a corresponding probability space and a random variable 

\subsection{Continuous Random Variable}
For a continuous random variable and a real number $x_0$
\[\P(X = x_0) = 0\]

\textbf{Theorem:} if $F_X$ is a CDF, it is piecewise differentiable 

\textbf{Probability density function (PDF):} $p_X(x) = F_X'(x)$

For a continuous random variable $X$,
\[F_X(x) = \int_{-\infty}^x p_X(t) \; dt\]

\textbf{Theorem:} For X a continuous random variable with PDF $p_X$, 
\[\int_{-\infty}^{\infty} p_X(t) \; dt = 1\]

\textbf{Normal Distribution:} $X \sim N(\mu, \sigma^2)$ (Normal distribution with mean $\mu$ and variance $\sigma^2$) if 
\[F_X(x) = \int_{-\infty}^x \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(t - \mu)^2}{2\sigma^2}} \; dt\]

\subsection{Discrete Random Variables}
For a discrete random variable X, its CDF is 
\[F_X(x) = \sum_{k=1}^K p_k \cdot \mathbf{1}_{[x_k, \infty)}(x)\]
where $\{p_k\}_{k=1}^K$ is the \textbf{probability mass function} and is the probability $\P(X = x_k)$

\textbf{Example:} $X \sim \text{Bernoulli}(p)$
\[F_X(x) = (1-p)\cdot \mathbf{1}_{[0, \infty)}(x) + p\cdot \mathbf{1}_{[1, \infty)}\]

\textbf{Poisson Distribution:} $X \sim \text{Pois}(\lambda)$ if 
\[F_X(x) = \sum_{k=1}^\infty \frac{\lambda^k e^{-\lambda}}{k!} \cdot \mathbf{1}_{[k, \infty)}(x)\]

\subsection{Expected Value (Mean)}
\textbf{Discrete Version:} IF $X$ is a discrete RV and $\sum_{k=0}^K |x_t| \cdot p_k < \infty$ (i.e. if the sum if absolutely convergent), then 
\[\E(X) = \sum_{k=0}^K x_k \cdot p_k\]

\textbf{Continuous version:} IF $X$ is a continuous RV then if $\int_{-\infty}^{\infty} |x| \cdot p_X(x)\; dx < \infty$ then
\[\E(X) = \int_{-\infty}^{\infty} x\cdot p_X(x)\; dx\]

For a permutation $\sigma$ and an absolutely convergent series, 
\[\E(X) = \sum_{k=0}^\infty x_k \cdot p_k = \sum_{k=0}^\infty x_{\sigma(k)} \cdot p_{\sigma(k)}\]
In other words, the order of summation does not matter. 

\subsection{Transformations of RV}
For a real-valued function $g(x)$, $g(X)$ is also a random variable.

Assuming the expected value exists, 
\begin{itemize}
    \item If X is discrete, 
    \[\E[g(X)] = \sum_{k=0}^K g(x_k) \cdot p_k\]
    \item If X is continuous,
    \[\E[g(X)] = \int_{-\infty}^{\infty} g(x) \cdot p_X(x)\; dx\]
\end{itemize}

\subsubsection*{Properties of Expected Values}
\begin{itemize}
    \item For a constant c, $\E(c) = c$
    \item For constants $a, b$, 
    \[\E[aX + b] = a\cdot \E(X) + b\]
    \item For $g_1(x), \, ..., g_J(x)$ as functions where $\E[g_k(x)]$ exists for all $k = 1, \, ..., J$, 
    \[\E[g_1(X) + \, ... + g_J(x)] = \E\left[\sum_{k=1}^J g_k(x)\right] = \sum_{k=1}^J \E[g_k(X)]\]
    (the expected value is linear)
\end{itemize}

\subsection{Variance}
For a RV $X$ that follows some distribution and generates numbers $X_1, \, ..., X_n$

\textbf{Sample average:}
\[\overline{X}_n := \frac{1}{n} \sum_{i=1}^n X_i\approx \E X\]

\textbf{Variance:} For a RV $X$ whose expected value exists, 
\[\Var X = \E[(x - \E X)^2]\]
this can also be written 
\[\Var X = \sum_{k=0}^K (x_k - \E X)^2 \cdot p_k \qquad (\text{X discrete})\]
\[\Var X = \int_{-\infty}^{\infty} (x - \E X)^2 \cdot p_X(x)\; dx \qquad (\text{X continuous})\]

\subsubsection*{Properties of Variance}
\begin{enumerate}
    \item $\Var X = \E[(X - \E X)^2] = \E(X^2) - (\E X)^2$
    \item $\Var(aX + b) = a^2 \Var(X)$
    \item For any constant c, $\Var c = 0$
    \item If $\Var X =0$ then there exists a c such that $\P(X = c) = 1$
\end{enumerate}

\subsection{The Law of Large Numbers (LLN)}
\textbf{Theorem:} Let $\{X_i\}_{i=1}^\infty$ be an infinitely long sequence of independently and identically distributed RVs defined on $(\Omega, \P)$, then 
\[\P\left(\{\omega \in \Omega: \lim_{n\to \infty} \frac{X_1(\omega) + X_2(\omega) +\, ... + X_n(\omega)}{n} = \E X_1\}\right) = 1\]
where $\E X_1 = \E X_2 = ...$ because the CDFs are equal (by identical distribution)

\textbf{Generalized Theorem:} Let $\{X_i\}_{i=1}^\infty$ be iid RVs defined on $(\Omega, \P)$. If $\E[g(X_1)]$ exists, then 
\[\P\left(\{\omega \in \Omega: \lim_{n\to \infty} \frac{g(X_1(\omega)) + g(X_2(\omega)) +\, ... + g(X_n(\omega))}{n} = \E[g(X_1)]\}\right) = 1\]
\
\subsection{Monte Carlo Integration}
If we seek to solve a very hard integral, e.g., 
\[I = \int_0^1 \cos^{-1}\left(\frac{\cos(\frac{\pi}{2}x)}{1 + 2\cos(\frac{\pi}{2}x)}\right)\; dx\]

We let $U \sim \text{Unif}(0, 1)$ whose PDF is $\mathbf{1}_{[0, 1)}(x)$. Denote the integrand $g(x)$. Then
\[I = \E[g(U)]\]

We can generate $X_1(\omega),\, X_2(\omega), ... \overset{iid}{\sim} \text{Unif}(0, 1)$ and with enough random values 
\[\overline{g(X_n)} = \frac{g(X_1) + g(X_2) + \, ... + g(X_n)}{n} \approx \E[g(X_1)] = \int_0^1 \cos^{-1}\left(\frac{\cos(\frac{\pi}{2}x)}{1 + 2\cos(\frac{\pi}{2}x)}\right)\; dx\]

For integrals with bounds $(a, b)$ rather than $(0, 1)$ we can use the same method but define a new random variable from $U \sim \text{Unif}(0, 1)$ where 
\[X = a + (b - a) \cdot U \sim \text{Unif}(a, b)\]

\subsection{Law of the Iterated Logarithm}
\textbf{Error:} $e_n(\omega) = \overline{g(X_n)} - \E[g(X_1)]$

\textbf{Theorem:} Let $X_1, X_2, \, ...$ be iid RVs on $(\Omega, \P)$ with $E X_1$ and $\Var X_1$ existing. Then (heuristically) 
\begin{align*}
    \P(\{\omega \in \Omega: |e_n(\omega)| &\leq \sqrt{\Var X_i \cdot \frac{2\log(\log n)}{n}}\}) \approx 1\\
    \P(\{\omega \in \Omega: |e_n(\omega)| &> \sqrt{\Var X_i \cdot \frac{2\log(\log n)}{n}}\}) \approx 0
\end{align*}

\subsection{Central Limit Theorem}
\textbf{Theorem:} Let $\{X_i\}_{i=1}^\infty$ be a sequence of iid RVs on $(\Omega, \P)$. Suppose $\E X_i$ and $\Var X_i$ exist. Define a sequence of random variables $\{G_n\}_{n=1}^\infty$ so that 
\[G_n(\omega) = \sqrt{n} \cdot e_n(\omega) = \sqrt{n} \cdot (\overline X_n(\omega) - \E X_1)\]
Then the CDf of $G_n$ converges to the CDF of $N(0, \Var X_1)$ as $n \to \infty$:
\[\lim_{n \to \infty} \P(G_n \leq x) = \int_{-\infty}^x \frac{1}{\sqrt{2\pi \cdot \Var X_1}} \cdot \exp\left(-\frac{t^2}{2\Var X_1}\right)\; dt\]

\textbf{Corollary:} Under the same conditions,
\[\frac{G_n(\omega)}{\sqrt{\Var X_1}} = \sqrt{n} \cdot \frac{\overline X_n(\omega) - \E X_1}{\sqrt{\Var X-1}} \; \dot \sim \; N(0, 1)\]

\subsubsection*{Proof of the CLT}
\textbf{Weak Convergence} $\mathbf{G_n \overset{w}{\to} G}$: A sequence $G_1, \, ..., G_n$ of RV converge weakly to a continuous RV $G$ if 
\[\lim_{n \to \infty} G_{G_n}(x) = F_G(x)\]

\textbf{Strong Convergence:} if 
\[\lim_{n \to \infty} G_n(\omega) = G(\omega) \quad \forall \omega \in \Omega\]

\textbf{Moment Generating Functions:} For a RV $X$, 
\[M_X(t) = \E[e^{tX}]\]
is the moment generating function. 

\textbf{k-th Moment of X:} 
\[\frac{d^k}{dt^k} M_X(0) = \E X^k\]

For a sequence of RVs $G_1, \, ..., G_n$ with continuous RV $G$,  
\[\lim_{n\to \infty} M_{G_n}(t) = M_G(t) \Longrightarrow G_n \overset{w}{\to} G\]

Then if $G \sim N(0, \Var X_1)$, proof of the CLT simply depends on proving the convergence of the moment-generating functions. 

\textbf{Some Lemmas:}
\begin{enumerate}
    \item For $S_n(\omega) = \sum_{i=1}^n X_i(\omega)$ with $X_i$ RVs, if $X_1, \, ..., X_n$ are independent 
    \[M_{S_n}(t) = \prod_{i=1}^n M_{X_i}(t)\]
    \item If the same RVs $X_1,\, ..., X_n$ are also identically distributed 
    \[M_{S_n}(t) = (M_{X_1}(t))^n\]
    \item For $\{C_n\}_{n=1}^\infty$ being a sequence of real-numbers for which $\lim_{n\to\infty} C_n = 0$, if $\lim_{n\to\infty} n\cdot C_n = \lambda$, then 
    \[\lim_{n\to\infty} (1 + C_n)^n = e^{\lambda}\]
    \item If $G \sim N(0, \sigma^2)$ then 
    \[M_G(t) = \exp(\frac{t^2 \sigma^2}{2})\]
\end{enumerate}

\textbf{Finally the proof:}
\begin{align*}
    G_n &= \sqrt{n} \left[\left(\frac{1}{n}\sum_{i=1}^n X_i\right) - \E X_1\right]\\
    &= \sqrt{n} \left[\left(\frac{1}{n}\sum_{i=1}^n X_i\right) - \frac{1}{n} \cdot n \cdot \E X_1\right]\\
    &= \frac{\sqrt{n}}{n} \left[\sum_{i=1}^n X_i - \sum_{i=1}^n \E X_1\right]\\
    &= \frac{\sqrt{n}}{n} \left[\sum_{i=1}^n (X_i - \E X_1)\right]\\
\end{align*}
Then 
\begin{align*}
    M_{G_n}(t) &= \E[e^{tG_n}] = \E[ \exp(\frac{t}{\sqrt{n}})\sum_{i=1}^n (X_i - \E X_1)]\\
    &= \E\left[\exp\left(\sum_{i=1}^n \frac{t}{\sqrt{n}}(X_i - \E X_1)\right)\right]\\
    &= \E\left[prod_{i=1}^n\exp\left(\sum_{i=1}^n \frac{t}{\sqrt{n}}(X_i - \E X_1)\right)\right]\quad (\text{by iid})\\
    &= \left(\E\left[\exp\left(\sum_{i=1}^n \frac{t}{\sqrt{n}}(X_i - \E X_1)\right)\right]\right)^n\\
    &= \left(\E\left[1 + \frac{t}{\sqrt{n}}(X_1 - \E X_1) + \frac{t^2}{2n}(X_1 - \E X_1)^2 + \sum_{k=3}^\infty \frac{t^k}{k!n^{k/2}}(X_1 - \E X_1)^k\right]\right)^n\\
    &= \left(1 + \underbrace{\frac{t^2}{2n}\Var X_1 +  \sum_{k=3}^\infty \frac{t^k}{k!n^{k/2}}(X_1 - \E X_1)^k}_{C_n}\right)^n\quad (\text{because } \E[X_1 - \E X_1] = 0)
\end{align*}

Using the lemmas above $\lim_{n\to\infty} C_n = 0$ and 
\[n\cdot C_n = \lambda = \frac{t^2}{2}\Var X_1 + \sum_{k=3}^\infty \frac{t^k}{k!n^{\frac{k}{2} - 1}}\E[(...)^k]\]

But when $k \geq 3, \; (\frac{k}{2} - 1) >0$ so 
\[\lim_{n\to \infty} n \cdot C_n = \frac{t^2}{\Var X_1} := \lambda\]
Then again by the lemmas
\[M_{G_n}(t) = (1 + C_n)^n \overset{n\to \infty}{\longrightarrow} e^{\lambda} = \exp(\frac{t^2}{2}\Var X_1)\]

But from the final lemma, the MGF of $N(0, \Var X_1)$ is 
\[M_G(t) = \exp(\frac{t^2}{2}\Var X_1)\] 

Thus, 
\[M_{G_n}(t) \overset{n\to \infty} M_G(t)\]
and 
\[G_n \overset{w}{\to} G \sim N(0, \Var X_1) \quad \blacksquare\]

\subsection{Error Bounds}
Let $\{X_i\}_{i=1}^\infty$ be a sequence of iid RVs on $(\Omega, \P)$. Suppose $\E X_i$ and $\Var X_i$ exist. 

From the law of the iterated logarithm, $|e_n(\omega) \leq \sqrt{2\log(\log n)} \cdot \sqrt{\frac{\Var X_1}{n}}$ with probability around 100\%. 

\begin{align*}
    &\P\left(|e_n(\omega)| \leq z \cdot \sqrt{\frac{\Var X_1}{n}}\right)\\
    &= \P(-z \leq \sqrt{n} \cdot \frac{e_n(\omega)}{\sqrt{\Var X_1}} \leq z)\\
    &= \P\left(\sqrt{n} \cdot \frac{e_n(\omega)}{\sqrt{\Var X_1}} \leq z\right) - \P\left(\sqrt{n} \cdot \frac{e_n(\omega)}{\sqrt{\Var X_1}} \leq -z\right)\\
    &\approx \Phi(z) - \Phi(-z) \qquad (CLT)\\
    &= 2\Phi(x) - 1
\end{align*}
Where $\Phi$ is the CDF of $N(0, 1)$.

Now let $z^*$ denote the positive real number such that $\Phi(z^*) = 0.975$ so 
\[\P\left(|e_n(\omega) \leq z^* \cdot \sqrt{\frac{\Var X_1}{n}}\right) \approx 2\Phi(z^*) - 1 = 0.95\] 

Generally, you can choose $z^*$ such that $\Phi(z^*) = 1- \frac{\alpha}{2}$ so $2\Phi(z^*) -1 = 1- \alpha$. Then $z^*$ os the ``$1 - \alpha/2$ quantile of $N(0, 1)$.'' 

All together, this gives
\[|e_n(\omega)| \leq z^* \cdot \sqrt{\frac{\Var X_1}{n}} \approx 1.96 \cdot \sqrt{\frac{\Var X_1}{n}}\]

\textbf{Conclusion:} using the CLT we can establish much tighter error bounds than the LIL approach at the cost of only 5\% confidence. 

\subsection{Random Vectors}
\textbf{Random Vector:} a column vector $\vec{X} = (X_1, X_2\, ..., X_n)^T$ defined on $(\Omega, \P)$ if each of its components is a RV. 

\textbf{CDF of a Random Vector:} an n-variable function 
\[F_{\vec{X}}(x_1, x_2, \, ..., x_n) = \P\left(\bigcap_{i=1}^n \{\omega \in \Omega: X_i(\omega) \leq x_i\}\right)\]

\textbf{Continuous Random Vector:} a random vector $\vec{X}$ if $F_{\vec{X}}$ is differentiable 

\textbf{The PDF of a Random Vector:}
\[p_{\vec{X}}(x_1, \, ..., x_n) = \frac{\partial}{\partial x_1}\frac{\partial}{\partial x_2}\,...\,\frac{\partial}{\partial x_n} \, F_{\vec{X}}(x_1, x_2,\, ..., x_n)\]

\textbf{Expected value of a Random vector:} If $\vec{X} = (X_1, \,..., X_n)^T$ is a continuous random vector with PDF $p_{\vec{X}}$, $g(\vec{x})$ is an n-variable function, $\int_{\R^n} |g(x_1\, ..., x_n)| \cdot p_{\vec{X}}(x_1\, ..., x_n) \; dx_1 ... dx_n <\infty$ then 
\[\E[g(\vec{X})] = \underbrace{\int_{-\infty}^{\infty} ... \int_{-\infty}^{\infty}}_{n \text{ integrals}} g(x_1\, ..., x_n) \cdot p_{\vec{X}}(x_1, \, ..., x_n) \; dx_1 ... dx_n\]

\section{Statistics}
\subsection{Statistical Models}
\textbf{Sample Data:} a collection $\{x_i\}_{i=1}^n = X_i(\omega^*)$ of deterministic numbers for some fixed $\omega^* \in \Omega$

\textbf{Sample size:} $n$ in the definition of data 

\textbf{$\mathfrak{F}$-based model:} Let $\mathfrak{F} = \{F_\theta\}_{\theta \in \Theta}$ be some family of real-valued functions satisfying the CDF properties. Then the $\mathfrak{F}$-based model is the assumption that there exists some ``true'' $\theta^* \in \Theta$ for which
\[X_1, X_2,\, ..., X_n \overset{iid}{\sim} F_{\theta^*}(x) = \P(X_1 \leq x)\]

\textbf{Parameter Space:} $\{F_\theta\}_{\theta \in \Theta}$, the family of functions from which we are selecting

\textbf{Parametric Model:} a model for which $\Theta$ is a subset of a finite-dimensional space

\textbf{Non-parametric Model:} a model for which $\Theta$ is a subset of an infinite-dimensional space

\textbf{Unspecified model:} a model for which the underlying assumption is incorrect (i.e. no true $\theta^*$ exists)

\textbf{Statistical inference:} the process of combining probability theory and data to infer the value of $\theta^*$

\subsection{Hypothesis Testing}
We assume the $\{F_\theta\}_{\theta \in \Theta}$-based model is correct. We then let $\Theta = \Theta_0 \cup \Theta_1$, giving us two hypthoses ($\Theta_0$ and $\Theta_1$ partition $\Theta$). Either:
\begin{enumerate}
    \item \textbf{The Null Hypothesis:} $H_0: \theta^* \in \Theta_0$ 
    \item \textbf{The Alternative Hypothesis:} $H_1: \theta^* \in \Theta_1$
\end{enumerate}

\textbf{Test:} For sample size $n$, a test is any function $T: \R^n \to \{0, 1\}$. If $T(\vec{x}) = 1$ we reject $H_0$. If it is 0, we accept $H_0$. As $T$ outputs in $\{0, 1\}$,
\[T(X_1(\omega), \,..., X_n(\omega)) = R(\omega) \sim \text{Bernoulli}(r)\]
where $r= \P(R = 1) = \E R$ 

\textbf{Type 1 Error:} the null hypothesis is true ($\theta^* \in \Theta_0$) but we reject it ($T = 1$)

\textbf{Type 2 Error:} the null hypothesis is false ($\theta^* \in \Theta_1$) but we fail to reject it ($T = 0$)

\subsubsection*{Criteria for a Good Test}
We define a function 
\[\beta_T(\theta) = \underbrace{\int_{-\infty}^{\infty} ...\int_{-\infty}^{\infty}}_{n} T(\xi_1, \, ..., \xi_n) \cdot \prod_{i=1}^n p(\xi_i|\theta) \; d\xi_1\, ...\, d\xi_n\]
where $p(\xi_i |\theta) = F_\theta'(\xi_i)$ so $\beta_T(\theta^*) = \E[T(\vec{X})]$

This function can be interpreted ``if $F_\theta$ is the true CDF, the probability of rejecting $H_0$ through $T$ is $\beta_T(\theta)$''

\textbf{Minimize Type 1 Error:} make $\sup_{\theta \in \Theta}$ (``the significance of T'') small 

\textbf{Minimize Type 2 Error:} make $\beta_T(\theta)$ large for every $\theta \in \Theta_1$

\subsection{Uniformly Most Powerful Test (UMP Test)}
\textbf{Definition:} Let $\alpha \in (0, 1)$ be pre-specified. Suppose $T^*$ is a test with significance $\alpha$ ($\sup_{\theta \in \Theta_0} \beta_{T^*}(\theta) = \alpha$). Then $T^*$ is said to be a UMP test with significance $\alpha$ if for all T for which $\sup_{\theta \in \Theta_0} \beta_T(\theta) = \alpha$
\[\beta_T(\theta) \leq \beta_{T^*}(\theta) \quad \forall \theta \in \Theta_1\]

\textbf{Neyman-Pearson Lemma:} With $\Theta = \{\theta_0\, , \theta_1\}, \; \Theta_0 = \{\theta_0\}, \; \Theta_1 = \{\theta_1\}$. Let $p(\xi | \theta) = F_\theta'(\xi)$ for all $\theta \in \Theta$. For any $\alpha \in (0, 1)$, the UMP test with significance alpha is 
\[T_\alpha(\xi_1, \, ..., \xi_n) = \mathbf{1}\left(\frac{\prod_{i=1}^n p(\xi_i | \theta_1)}{\prod_{i=1}^n p(\xi_i | \theta_0)} > C_\alpha\right)\]
where $C_\alpha$ is the solution to $\beta_{T_{NP, \, \alpha}}(\theta_0) = \alpha$

\subsection{The Maximum Likelihood Estimator}
\textbf{Point Estimating:} the process of estimating $\theta^*$ in the $\mathfrak{F}$-based model, usually via MLE, the method of moments, or mean squared estimation. 

\textbf{The Likelihood Function:} We assume the $\{F_\theta\}_{\theta \in \Theta}$-based model is correct. We assume that $F_\theta$ is piecewise differentiable for all $\theta$ and we have a collection of given, fixed, deterministic data $D = \{x_i\}_{i=1}^n$. Then 
\[L(\theta | D) = \prod_{i=1}^n p(x_i | \theta)\]

\textbf{The MLE:} We select the $\theta$ which maximized the likelihood function 
\[\hat \theta_{MLE} = \underset{\theta \in \Theta}{\arg \max} \; L(\theta | D)\]
which by the consistency property is approximately equal to $\theta^*$

\textbf{Loss function:} $-L(\theta | D)$ as 
\[\underset{\theta \in \Theta}{\arg \max} \; L(\theta | D) = \underset{\theta \in \Theta}{\arg \min} \; -L(\theta | D)\]

\textbf{Log-likelihood function:}
\[l(\theta  | D) = \log L(\theta | D) = \sum_{i=1}^n \log p(x_i | \theta)\]

\subsubsection*{Calculating the MLE}
\textbf{Example:} $\Theta = \R$ and $F_{\theta}$ is the CDF of $N(0,1)$. 
\begin{align*}
    p(\xi | \theta) &=\frac{1}{\sqrt{2\pi}} \exp(-\frac{(\xi - \theta)^2}{2})\\
    l(\theta | D) &= \sum_{i=1}^n \log p(x_i | \theta)\\
    &= \sum_{i=1}^n \log\left(\frac{1}{\sqrt{2\pi}} \exp(-\frac{(x_i - \theta)^2}{2})\right)\\
    &= \sum_{i=1}^n \left(-\frac{1}{2}\log(2\pi) - \frac{1}{2}(x_i - \theta)^2\right)\\
    &= -\frac{n}{2}\log(2\pi) - \sum_{i=1}^n \frac{1}{2}(x_i - \theta)^2
\end{align*}
The max will occur at the critical point so 
\begin{align*}
    \hat \theta &= \frac{\partial}{\partial \theta} l(\theta | D) = 0\\
    &= \frac{\partial}{\partial \theta} \left(-\frac{n}{2}\log(2\pi) - \sum_{i=1}^n \frac{1}{2}(x_i - \theta)^2\right)\
    &= \sum_{i=1}^N (x_i - \theta)\\
    &= -n \cdot \theta + \sum_{i=1}^n x_i = 0\\
    &\implies \theta = \frac{1}{n} \sum_{i=1}^n x_i = \overline{x}_n
\end{align*}

From the second derivative 
\[\frac{\partial^2}{\partial \theta^2} l(\theta| D) = -n < 0\]
so this is the maximum 
\[\hat \theta_{MLE} = \overline x_n\]

\textbf{Consistency of the MLE:} Supposing that on $(\Omega, \P)$, the $\{F_\theta\}_{\theta \in \Theta}$-based model is correct and the ``regularity conditions'' apply, 
\[\forall \varepsilon > 0, \quad \lim_{n\to \infty} \P(\{\omega\in \Omega: |\hat \theta(\omega) - \theta^*| < \varepsilon\}) = 1 \]
where $\hat \theta_n(\omega) = \hat \theta_{MLE}(X_1(\omega), \, ..., X_n(\omega))$

\textbf{Asymptotic normality of the MLE:} under the same conditions as consistency, 
\[\sqrt{n} (\hat \theta_n - \theta^*) \dot \sim N(0, \frac{1}{I(\theta^*)})\]
where the ``Fisher Information'' $I(\theta)$ is 
\[I(\theta) = \int_{-\infty}^{\infty} \left(\frac{\partial}{\partial \theta} \log p(\xi | \theta)^2\right)^2 \cdot p(\xi | \theta)\; d\xi\]

\end{document}